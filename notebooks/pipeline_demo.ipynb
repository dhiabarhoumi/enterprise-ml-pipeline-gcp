{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise ML Pipeline on GCP - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the usage of the Enterprise ML Pipeline built on Google Cloud Platform. It covers the following steps:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Generating and uploading synthetic data to BigQuery\n",
    "3. Data validation and preprocessing\n",
    "4. Running the ML pipeline locally\n",
    "5. Model evaluation\n",
    "6. Making predictions with the deployed model\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.data.bigquery_utils import BigQueryClient\n",
    "from src.data.data_validation import DataValidator\n",
    "from src.pipeline.config import PipelineConfig\n",
    "from src.models.evaluation import ModelEvaluator\n",
    "from src.serving.prediction import PredictionService\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display TensorFlow version\n",
    "print(f'TensorFlow version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure GCP Project\n",
    "\n",
    "Set your Google Cloud Platform project ID and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your GCP project ID and region\n",
    "PROJECT_ID = 'your-gcp-project-id'  # Replace with your project ID\n",
    "REGION = 'us-central1'  # Replace with your preferred region\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_REGION'] = REGION\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Create pipeline config\n",
    "config = PipelineConfig(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    pipeline_name='retail-sales-pipeline'\n",
    ")\n",
    "\n",
    "print(f'Project ID: {PROJECT_ID}')\n",
    "print(f'Region: {REGION}')\n",
    "print(f'Pipeline root: {config.pipeline_root}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate and Upload Synthetic Data\n",
    "\n",
    "Let's generate synthetic retail data and upload it to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.upload_to_bigquery import generate_retail_data\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 10000\n",
    "df = generate_retail_data(num_samples)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f'Generated {num_samples} synthetic retail records')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of total_amount (target variable)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['total_amount'], kde=True)\n",
    "plt.title('Distribution of Total Amount')\n",
    "plt.xlabel('Total Amount')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Visualize relationship between quantity and total_amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='quantity', y='total_amount', data=df, alpha=0.5)\n",
    "plt.title('Quantity vs Total Amount')\n",
    "plt.xlabel('Quantity')\n",
    "plt.ylabel('Total Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "bq_client = BigQueryClient(project_id=PROJECT_ID)\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "dataset_id = 'retail_dataset'\n",
    "bq_client.create_dataset(dataset_id)\n",
    "\n",
    "# Upload data to BigQuery\n",
    "table_id = 'retail_sales'\n",
    "bq_client.upload_dataframe_to_table(df, dataset_id, table_id)\n",
    "\n",
    "print(f'Data uploaded to {PROJECT_ID}.{dataset_id}.{table_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Validation and Preprocessing\n",
    "\n",
    "Let's validate the data using TensorFlow Data Validation (TFDV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data validator\n",
    "data_validator = DataValidator()\n",
    "\n",
    "# Generate statistics for the dataset\n",
    "stats = data_validator.generate_statistics(df)\n",
    "\n",
    "# Infer schema from statistics\n",
    "schema = data_validator.infer_schema(stats)\n",
    "\n",
    "# Display statistics and schema\n",
    "data_validator.display_statistics(stats)\n",
    "data_validator.display_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and evaluation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set size: {len(train_df)}')\n",
    "print(f'Evaluation set size: {len(eval_df)}')\n",
    "\n",
    "# Generate statistics for training and evaluation sets\n",
    "train_stats = data_validator.generate_statistics(train_df)\n",
    "eval_stats = data_validator.generate_statistics(eval_df)\n",
    "\n",
    "# Compare statistics\n",
    "data_validator.compare_statistics(train_stats, eval_stats, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running the ML Pipeline Locally\n",
    "\n",
    "Now, let's run the ML pipeline locally using TFX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and evaluation data to CSV for the pipeline\n",
    "data_dir = os.path.join('..', 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(data_dir, 'train.csv')\n",
    "eval_path = os.path.join(data_dir, 'eval.csv')\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "print(f'Training data saved to {train_path}')\n",
    "print(f'Evaluation data saved to {eval_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline runner\n",
    "from src.pipeline.run_pipeline import create_pipeline, run_pipeline\n",
    "\n",
    "# Run the pipeline locally\n",
    "# Note: This cell will execute the pipeline, which may take some time\n",
    "# Uncomment the following lines to run the pipeline\n",
    "\n",
    "# run_pipeline(\n",
    "#     config=config,\n",
    "#     data_path=data_dir,\n",
    "#     mode='local',\n",
    "#     enable_cache=True\n",
    "# )\n",
    "\n",
    "print('To run the pipeline, execute the following command in the terminal:')\n",
    "print(f'python ../src/pipeline/run_pipeline.py --project_id={PROJECT_ID} --region={REGION} --data_path={data_dir} --mode=local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = os.path.join(config.serving_model_dir, 'latest')\n",
    "\n",
    "# Check if model exists\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f'Model loaded from {model_path}')\n",
    "    \n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "else:\n",
    "    print(f'Model not found at {model_path}. Please run the pipeline first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If model exists, evaluate it on the evaluation dataset\n",
    "if 'model' in locals():\n",
    "    # Prepare evaluation data\n",
    "    # Note: This is a simplified evaluation. In practice, you would need to preprocess the data\n",
    "    # using the same transformations applied during training.\n",
    "    \n",
    "    # Initialize model evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "    \n",
    "    # Generate evaluation report\n",
    "    # This is a placeholder - in a real scenario, you would use the proper transformed data\n",
    "    print('To evaluate the model properly, you should use the TFX Evaluator component outputs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Making Predictions with the Deployed Model\n",
    "\n",
    "Let's use the model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input data\n",
    "sample_input = {\n",
    "    'quantity': 5,\n",
    "    'unit_price': 50.0,\n",
    "    'discount': 0.1,\n",
    "    'customer_age': 35,\n",
    "    'transaction_hour': 14,\n",
    "    'customer_id': 'CUST_1001',\n",
    "    'product_id': 'PROD_5432',\n",
    "    'customer_gender': 'M',\n",
    "    'store_id': 'STORE_01',\n",
    "    'payment_method': 'Credit Card',\n",
    "    'customer_segment': 'Regular',\n",
    "    'transaction_day': 'Monday',\n",
    "    'transaction_month': 'January'\n",
    "}\n",
    "\n",
    "# If model exists locally, use PredictionService\n",
    "if os.path.exists(model_path):\n",
    "    prediction_service = PredictionService(model_path)\n",
    "    \n",
    "    # Make prediction\n",
    "    try:\n",
    "        prediction = prediction_service.predict_single(sample_input)\n",
    "        print(f'Predicted total amount: ${prediction:.2f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error making prediction: {e}')\n",
    "else:\n",
    "    print('Model not available locally. You can deploy it to Cloud Run for online predictions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the Model to Cloud Run\n",
    "\n",
    "To deploy the model to Cloud Run for online predictions, you can use the deployment script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the command to deploy the model\n",
    "print('To deploy the model to Cloud Run, execute the following command in the terminal:')\n",
    "print(f'bash ../scripts/deploy_model.sh {PROJECT_ID} {REGION} {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the key components of our Enterprise ML Pipeline on GCP:\n",
    "\n",
    "1. Setting up the environment and configuring GCP resources\n",
    "2. Generating and uploading synthetic retail data to BigQuery\n",
    "3. Validating and preprocessing the data using TensorFlow Data Validation\n",
    "4. Running the TFX pipeline locally\n",
    "5. Evaluating the trained model\n",
    "6. Making predictions with the deployed model\n",
    "\n",
    "This pipeline demonstrates a production-ready approach to machine learning workflows, incorporating best practices for data validation, model training, evaluation, and deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
